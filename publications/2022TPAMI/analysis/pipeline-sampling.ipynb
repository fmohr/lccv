{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from commons import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing of the Used Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "searchspace_file = \"experiment-controller/python/singularity/searchspace.json\"\n",
    "searchspace = json.load(open(searchspace_file))\n",
    "dummy_sampler = PipelineSampler(searchspace_file, None, None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\medskip\\noindent\\textbf{data-pre-processors}: \n",
      "MinMaxScaler, Normalizer, PowerTransformer, QuantileTransformer, RobustScaler, StandardScaler, VarianceThreshold\n",
      "\n",
      "\n",
      "\\medskip\\noindent\\textbf{feature-pre-processors}: \n",
      "FastICA, FeatureAgglomeration, GenericUnivariateSelect, KernelPCA, Nystroem, PCA, PolynomialFeatures, RBFSampler, SelectPercentile\n",
      "\n",
      "\n",
      "\\medskip\\noindent\\textbf{classifiers}: \n",
      "BernoulliNB, DecisionTreeClassifier, ExtraTreesClassifier, GaussianNB, GradientBoostingClassifier, KNeighborsClassifier, LinearDiscriminantAnalysis, MLPClassifier, MultinomialNB, PassiveAggressiveClassifier, QuadraticDiscriminantAnalysis, RandomForestClassifier, SGDClassifier, SVC\n"
     ]
    }
   ],
   "source": [
    "for i, step in enumerate(searchspace):\n",
    "    print(\"\\n\\n\\\\medskip\\\\noindent\\\\textbf{\" + step[\"name\"] + \"s}: \")\n",
    "    algnames = []\n",
    "    for comp in step[\"components\"]:\n",
    "        algname = comp[\"class\"]\n",
    "        algnames.append(algname[algname.rindex(\".\") + 1:])\n",
    "    print(\", \".join(np.unique(algnames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing away the 10 pipeline sequences for the 10 seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(10):\n",
    "    out = \"\"\n",
    "    sampler = PipelineSampler(searchspace_file, None, None, seed, dp_proba=0.5, fp_proba = 0.5)\n",
    "    for i in range(200):\n",
    "        pl = sampler.sample(do_build=False)\n",
    "        desc = []\n",
    "        for step in pl:\n",
    "            name = step[0]\n",
    "            comp = step[1][0]\n",
    "            params = step[1][1]\n",
    "            desc.append(f\"{comp['class'][comp['class'].rindex('.') + 1:]} ({params})\")\n",
    "        out += f\"{i + 1}: {', '.join(desc)}\\n\"\n",
    "    open(f\"pipeline-sequences/pipelines-{seed}.txt\", \"w\").write(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
